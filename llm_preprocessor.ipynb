{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e942671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import logging\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4647c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url',\n",
       " 'title',\n",
       " 'publish_date',\n",
       " 'authors',\n",
       " 'scraped_at',\n",
       " 'keywords',\n",
       " 'text',\n",
       " 'text_length']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"output\\\\articles_scraped.xlsx\")\n",
    "#df.shape\n",
    "df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0263b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "from transformers import GPT2TokenizerFast\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ad6d789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/gpt-4o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c56fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=\"GROQ_API_KEY\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6458888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086122ac",
   "metadata": {},
   "source": [
    "#### OpenAI (gpt4oMini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 16:50:13,454 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to Output\\Ergebnis_test\\2025-07-06_16-50-13_zs_gpt4o_mini.txt\n"
     ]
    }
   ],
   "source": [
    "def get_articles_with_metadata(df, num_articles=5, offset=0):\n",
    "    articles = df.iloc[offset:offset + num_articles][['url',\n",
    " 'title',\n",
    " 'publish_date',\n",
    " 'authors',\n",
    " 'scraped_at',\n",
    " 'keywords',\n",
    " 'text',\n",
    " 'text_length']].to_dict(orient='records')\n",
    "    return articles\n",
    "\n",
    "# Example to obtain the first 5 articles\n",
    "articles = get_articles_with_metadata(df, num_articles=5, offset=0)\n",
    "\n",
    "# Using metadata to build article text\n",
    "articles_text = \"\\n\\n\".join(\n",
    "    [f\"### Article {i+1}:\\nTitle: {article['title']}\\nURL: {article['url']}\\nPublish_date: {article['publish_date']}\\n\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Dynamically generate the ‘Article Summaries’ section with numbers\n",
    "article_summaries = \"\\n\\n\".join(\n",
    "    [f\"\"\"### {i+1}. \n",
    "     \n",
    "     **Title**: {article['title']}\n",
    "   **Link to source**: {article['url']} \n",
    "   **Publish_date**: {article['publish_date']}\n",
    "   **Summary**: Summarize the key points of the article in 2-3 sentences.\n",
    "   **Category**: Categorize the article into one or more retail-focused categories:[\"product_innovation\", \"pricing_strategy\", \"market_expansion\", \"retail_tech\", \"sustainability\", \"supply_chain_risk\", \"competitor_moves\", \"consumer_trends\", \"others\"].\n",
    "   **Entity Extraction**: Extract and identify brands/compagnies,  products, regions/markets, Financial Data, dates\n",
    "   **sentiments**:Analyze the overall sentiment: Positive, Negative, or Neutral, Market Sentiment, Competitive Sentiment, Supply Chain Sentiment. Provide a brief justification.\n",
    "   **trend detection **:Identify whether the article includes any of the following:Product launches or innovations, Pricing or promotion strategies, Store openings, expansions, or mergers, Retail technology integrations, Sustainability initiatives, Supply chain risks: delays, shortages, geopolitical events\n",
    "   **predictive intelligence **:Predict whether the article suggests:Short-term Implications, Medium-term Trends, Long-term Forecasts, Recommendation Priority, An emerging trend, A supply chain risk, A market shift, Or none\n",
    "   **Business Impact Scoring**: Rate the potential impact of the article on the business on a scale of 1 to 10, where 1 is low impact and 10 is high impact. Provide a brief justification.\n",
    "   \n",
    "   \n",
    "   \"\"\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert retail intelligence analyst working for a major retail/FMCG company that serves 15,000 suppliers and 200 retailers. Your role is to analyze news articles and extract actionable business intelligence that will help stakeholders make informed decisions about competitive positioning, supply chain management, and market opportunities.\n",
    "\n",
    "## ROLE\n",
    "Act as a Senior Retail Intelligence Analyst with expertise in:\n",
    "- Competitive intelligence and market analysis\n",
    "- Supply chain risk assessment\n",
    "- Predictive trend analysis\n",
    "- FMCG/Retail industry dynamics\n",
    "- Business impact evaluation\n",
    "\n",
    "## ACTION\n",
    "Analyze the provided news article and perform the following comprehensive analysis:\n",
    "\n",
    "### 1. INTELLIGENT CLASSIFICATION\n",
    "Categorize the article into ONE primary category:\n",
    "- **product_innovation**: New products, launches, and innovations\n",
    "- **pricing_strategy**: Pricing strategies, promotions, and competitive pricing\n",
    "- **market_expansion**: Store openings, market expansion, and geographic growth\n",
    "- **supply_chain**: Supply chain disruptions, logistics, and sourcing issues\n",
    "- **technology_retail**: Retail technology, digital transformation, and e-commerce\n",
    "- **sustainability**: Sustainability initiatives and ESG practices\n",
    "- **consumer_behavior**: Consumer trends, preferences, and behavioral shifts\n",
    "- **regulatory_compliance**: Regulatory changes, compliance, and policy impacts\n",
    "- **competitive_intelligence**: Competitor moves, market share, and strategic partnerships\n",
    "- **risk_management**: Geopolitical risks, economic factors, and crisis management\n",
    "\n",
    "### 2. ENTITY EXTRACTION\n",
    "Extract and identify:\n",
    "- **Brands/Companies**: All mentioned retail brands, suppliers, competitors\n",
    "- **Products**: Specific products, categories, or product lines\n",
    "- **Regions/Markets**: Geographic locations, countries, cities, markets\n",
    "- **Key Figures**: Important people, executives, industry leaders\n",
    "- **Financial Data**: Revenue, sales figures, market share, investments\n",
    "- **Dates**: Important dates, timelines, deadlines\n",
    "\n",
    "### 3. RETAIL-SPECIFIC SENTIMENT ANALYSIS\n",
    "Assess sentiment across three dimensions:\n",
    "- **Market Sentiment**: Positive/Negative/Neutral impact on market conditions\n",
    "- **Competitive Sentiment**: Advantage/Disadvantage/Neutral for competitive positioning\n",
    "- **Supply Chain Sentiment**: Stable/Risk/Opportunity for supply chain operations\n",
    "\n",
    "### 4. TREND DETECTION & INNOVATION ANALYSIS\n",
    "Identify:\n",
    "- **Emerging Trends**: New consumer behaviors, market shifts, technological adoption\n",
    "- **Innovation Signals**: R&D developments, patent filings, breakthrough technologies\n",
    "- **Product Launches**: New product introductions, line extensions, category expansions\n",
    "- **Market Disruptions**: Business model changes, new market entrants, industry shifts\n",
    "\n",
    "### 5. SUPPLY CHAIN RISK ASSESSMENT\n",
    "Evaluate:\n",
    "- **Disruption Risks**: Logistics issues, transportation problems, warehouse challenges\n",
    "- **Supplier Issues**: Supplier bankruptcies, quality problems, capacity constraints\n",
    "- **Geopolitical Risks**: Trade tensions, sanctions, regulatory changes\n",
    "- **Economic Factors**: Inflation, currency fluctuations, commodity price changes\n",
    "\n",
    "### 6. PREDICTIVE INTELLIGENCE\n",
    "Based on the information, predict:\n",
    "- **Short-term Implications** (1-3 months): Immediate business impacts\n",
    "- **Medium-term Trends** (3-12 months): Evolving market dynamics\n",
    "- **Long-term Forecasts** (1-3 years): Strategic industry shifts\n",
    "- **Recommendation Priority**: High/Medium/Low urgency for stakeholder action\n",
    "\n",
    "### 7. BUSINESS IMPACT SCORING\n",
    "Rate the criticality on a scale of 1-10:\n",
    "- **Competitive Impact**: How significantly this affects competitive positioning\n",
    "- **Supply Chain Impact**: Risk level for supply chain operations\n",
    "- **Market Opportunity**: Potential for new business opportunities\n",
    "- **Strategic Importance**: Overall relevance to business strategy\n",
    "\n",
    "\n",
    "## FORMAT\n",
    "\n",
    "### Here's how I'd like you to return my results:\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "Welcome to our weekly newsletter, where we provide you with a brief overview summary of significant events. \n",
    "\n",
    "### Article Summaries:\n",
    "\n",
    "\n",
    "{article_summaries}\n",
    "\n",
    "Stay tuned for our next issue as we delve deeper into the evolving challenges and opportunities shaping the retail landscape.\n",
    "\n",
    "Warm regards,\n",
    "Your retail.ai team\n",
    "\n",
    "\n",
    "\n",
    "##There are several aspects we look for in a good summary:\n",
    " **Consistency**: A good summary should be consistent with the article. Please do not add any new information to the summary, even if you know more about the article's content. For example, if the article does mention the first name of 'President Biden', please do not use the full name 'Joe Biden' in your summary.\n",
    " **Relevance**: A good summary should only include important information from the article. In general, you should include the key points you highlighted in your summary, but feel free to omit details you think are unnecessary to convey the main idea of the article.\n",
    " **Conciseness**: A good summary should be short and to the point. But it is okay if the summary is shorter when the article is short.\n",
    " **Coherence**: A good summary is more than just a list of sentences. It should be a coherent paragraph that makes sense.\n",
    "\n",
    "\n",
    "## TONE\n",
    "Maintain a professional, analytical, and strategic tone throughout your analysis. Be:\n",
    "- **Objective**: Base conclusions on factual evidence from the text\n",
    "- **Actionable**: Provide insights that can drive business decisions\n",
    "- **Comprehensive**: Cover all relevant aspects without being verbose\n",
    "- **Strategic**: Focus on business implications and competitive advantages\n",
    "- **Precise**: Use specific retail/FMCG terminology and metrics\n",
    "\n",
    "\n",
    "## CONSTRAINTS\n",
    "- Only analyze information explicitly mentioned in the provided article\n",
    "- If information is not available for a section, use \"null\" or empty arrays\n",
    "- Maintain consistency in entity naming (e.g., \"Walmart\" not \"Wal-Mart\")\n",
    "- Prioritize actionable insights over general observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Input Articles:\n",
    "```{articles_text}``` \n",
    "\"\"\"\n",
    "###################\n",
    "\n",
    "# Measuring latency\n",
    "start_time = time.time()\n",
    "\n",
    "################\n",
    "# Générer la réponse\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini-2024-07-18\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt_template}],\n",
    "    #prompt=prompt_template,\n",
    "    max_tokens=8096,\n",
    "    top_p=0.5,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "#######################\n",
    "# Calculate latency and total processing time\n",
    "latency = time.time() - start_time\n",
    "#print(chat_completion.choices[0].message.content)\n",
    "#output_text = chat_completion.choices[0].message.content\n",
    "output_text = response.choices[0].message.content\n",
    "\n",
    "# Number of tokens generated in the response\n",
    "output_tokens = len(output_text.split())\n",
    "\n",
    "\n",
    "# Calculate tokens for input and output\n",
    "input_tokens = tokenizer(prompt_template)['input_ids']\n",
    "output_tokens = tokenizer(output_text)['input_ids']\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_input_tokens = len(input_tokens)\n",
    "num_output_tokens = len(output_tokens)\n",
    "\n",
    "# Calculate word length for input and output\n",
    "input_word_count = len(prompt_template.split())\n",
    "output_word_count = len(output_text.split())\n",
    "\n",
    "\n",
    "# Price calculation\n",
    "# Price in USD per million input and output  tokens\n",
    "price_per_million_input_tokens = 0.15  \n",
    "price_per_million_output_tokens = 0.60  \n",
    "\n",
    "\n",
    "# Calculation of entry and exit costs\n",
    "input_cost = (num_input_tokens / 1_000_000) * price_per_million_input_tokens\n",
    "output_cost = (num_output_tokens / 1_000_000) * price_per_million_output_tokens\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "\n",
    "# Define the folder path and create folders if necessary\n",
    "output_dir = os.path.join(\"Output\", \"Ergebnis_test\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the file name with the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"{current_time}_zs_gpt4o_mini.txt\"\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Save the result in the text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(output_text)\n",
    "    \n",
    "\n",
    "#print(f\"Output saved to {file_path}\")\n",
    "print(f\"Output saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Function to save the analysis in an Excel file\n",
    "\n",
    "def save_analysis_to_excel(model_name, context_window, output_speed, latency, price, output_price, input_price, time_period, num_input_tokens, num_output_tokens, input_word_count, output_word_count, base_dir=\"LLM_Analysis\"):\n",
    "    data = {\n",
    "        \"Context Window\": [context_window],\n",
    "        \"Output Speed (tokens/s)\": [output_speed],\n",
    "        \"Latency (seconds)\": [latency],\n",
    "        \"Total Cost ($)\": [price],\n",
    "        \"Output Price ($ per M tokens)\": [output_price],\n",
    "        \"Input Price ($ per M tokens)\": [input_price],\n",
    "        \"Time Period (seconds)\": [time_period],\n",
    "        \"Input Tokens\": [num_input_tokens],\n",
    "        \"Output Tokens\": [num_output_tokens],\n",
    "        \"Input Word Count\": [input_word_count],\n",
    "        \"Output Word Count\": [output_word_count]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    # Create the path to the Excel file\n",
    "    output_dir = os.path.join(base_dir, model_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = f\"{model_name}_analysis_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    try:\n",
    "        df.to_excel(file_path, sheet_name=model_name[:31], index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "           \n",
    "        \n",
    "            \n",
    "\n",
    "# An example of use\n",
    "context_window = 1047576  # Example of the size of the context window\n",
    "output_speed = num_output_tokens / latency  # Tokens per second\n",
    "time_period = latency  # Total time in seconds\n",
    "\n",
    "model_name = f\"zs_gpt4o_mini{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "save_analysis_to_excel(\n",
    "    model_name=model_name,\n",
    "    context_window=context_window,\n",
    "    output_speed=output_speed,\n",
    "    latency=latency,\n",
    "    price=total_cost,\n",
    "    output_price=price_per_million_output_tokens,\n",
    "    input_price=price_per_million_input_tokens,\n",
    "    time_period=time_period,\n",
    "    num_input_tokens=num_input_tokens,\n",
    "    num_output_tokens=num_output_tokens,\n",
    "    input_word_count=input_word_count,\n",
    "    output_word_count=output_word_count\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3156f6b3",
   "metadata": {},
   "source": [
    "## OpenAI (gpt41Mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6ff86faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 16:35:39,159 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, a gentle unicorn with sparkling hooves and a rainbow mane found a cozy cloud to sleep on beneath the twinkling stars.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-nano-2025-04-14\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a one-sentence bedtime story about a unicorn.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 16:45:10,700 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to Output\\Ergebnis_test\\2025-07-06_16-45-10_zs_gpt41_mini.txt\n"
     ]
    }
   ],
   "source": [
    "def get_articles_with_metadata(df, num_articles=5, offset=0):\n",
    "    articles = df.iloc[offset:offset + num_articles][['url',\n",
    " 'title',\n",
    " 'publish_date',\n",
    " 'authors',\n",
    " 'scraped_at',\n",
    " 'keywords',\n",
    " 'text',\n",
    " 'text_length']].to_dict(orient='records')\n",
    "    return articles\n",
    "\n",
    "# Example to obtain the first 5 articles\n",
    "articles = get_articles_with_metadata(df, num_articles=5, offset=0)\n",
    "\n",
    "# Using metadata to build article text\n",
    "articles_text = \"\\n\\n\".join(\n",
    "    [f\"### Article {i+1}:\\nTitle: {article['title']}\\nURL: {article['url']}\\nPublish_date: {article['publish_date']}\\n\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Dynamically generate the ‘Article Summaries’ section with numbers\n",
    "article_summaries = \"\\n\\n\".join(\n",
    "    [f\"\"\"### {i+1}. \n",
    "     \n",
    "     **Title**: {article['title']}\n",
    "   **Link to source**: {article['url']} \n",
    "   **Publish_date**: {article['publish_date']}\n",
    "   **Summary**: Summarize the key points of the article in 2-3 sentences.\n",
    "   **Category**: Categorize the article into one or more retail-focused categories:[\"product_innovation\", \"pricing_strategy\", \"market_expansion\", \"retail_tech\", \"sustainability\", \"supply_chain_risk\", \"competitor_moves\", \"consumer_trends\", \"others\"].\n",
    "   **Entity Extraction**: Extract and identify brands/compagnies,  products, regions/markets, Financial Data, dates\n",
    "   **sentiments**:Analyze the overall sentiment: Positive, Negative, or Neutral, Market Sentiment, Competitive Sentiment, Supply Chain Sentiment. Provide a brief justification.\n",
    "   **trend detection **:Identify whether the article includes any of the following:Product launches or innovations, Pricing or promotion strategies, Store openings, expansions, or mergers, Retail technology integrations, Sustainability initiatives, Supply chain risks: delays, shortages, geopolitical events\n",
    "   **predictive intelligence **:Predict whether the article suggests:Short-term Implications, Medium-term Trends, Long-term Forecasts, Recommendation Priority, An emerging trend, A supply chain risk, A market shift, Or none\n",
    "   **Business Impact Scoring**: Rate the potential impact of the article on the business on a scale of 1 to 10, where 1 is low impact and 10 is high impact. Provide a brief justification.\n",
    "   \n",
    "   \n",
    "   \"\"\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert retail intelligence analyst working for a major retail/FMCG company that serves 15,000 suppliers and 200 retailers. Your role is to analyze news articles and extract actionable business intelligence that will help stakeholders make informed decisions about competitive positioning, supply chain management, and market opportunities.\n",
    "\n",
    "## ROLE\n",
    "Act as a Senior Retail Intelligence Analyst with expertise in:\n",
    "- Competitive intelligence and market analysis\n",
    "- Supply chain risk assessment\n",
    "- Predictive trend analysis\n",
    "- FMCG/Retail industry dynamics\n",
    "- Business impact evaluation\n",
    "\n",
    "## ACTION\n",
    "Analyze the provided news article and perform the following comprehensive analysis:\n",
    "\n",
    "### 1. INTELLIGENT CLASSIFICATION\n",
    "Categorize the article into ONE primary category:\n",
    "- **product_innovation**: New products, launches, and innovations\n",
    "- **pricing_strategy**: Pricing strategies, promotions, and competitive pricing\n",
    "- **market_expansion**: Store openings, market expansion, and geographic growth\n",
    "- **supply_chain**: Supply chain disruptions, logistics, and sourcing issues\n",
    "- **technology_retail**: Retail technology, digital transformation, and e-commerce\n",
    "- **sustainability**: Sustainability initiatives and ESG practices\n",
    "- **consumer_behavior**: Consumer trends, preferences, and behavioral shifts\n",
    "- **regulatory_compliance**: Regulatory changes, compliance, and policy impacts\n",
    "- **competitive_intelligence**: Competitor moves, market share, and strategic partnerships\n",
    "- **risk_management**: Geopolitical risks, economic factors, and crisis management\n",
    "\n",
    "### 2. ENTITY EXTRACTION\n",
    "Extract and identify:\n",
    "- **Brands/Companies**: All mentioned retail brands, suppliers, competitors\n",
    "- **Products**: Specific products, categories, or product lines\n",
    "- **Regions/Markets**: Geographic locations, countries, cities, markets\n",
    "- **Key Figures**: Important people, executives, industry leaders\n",
    "- **Financial Data**: Revenue, sales figures, market share, investments\n",
    "- **Dates**: Important dates, timelines, deadlines\n",
    "\n",
    "### 3. RETAIL-SPECIFIC SENTIMENT ANALYSIS\n",
    "Assess sentiment across three dimensions:\n",
    "- **Market Sentiment**: Positive/Negative/Neutral impact on market conditions\n",
    "- **Competitive Sentiment**: Advantage/Disadvantage/Neutral for competitive positioning\n",
    "- **Supply Chain Sentiment**: Stable/Risk/Opportunity for supply chain operations\n",
    "\n",
    "### 4. TREND DETECTION & INNOVATION ANALYSIS\n",
    "Identify:\n",
    "- **Emerging Trends**: New consumer behaviors, market shifts, technological adoption\n",
    "- **Innovation Signals**: R&D developments, patent filings, breakthrough technologies\n",
    "- **Product Launches**: New product introductions, line extensions, category expansions\n",
    "- **Market Disruptions**: Business model changes, new market entrants, industry shifts\n",
    "\n",
    "### 5. SUPPLY CHAIN RISK ASSESSMENT\n",
    "Evaluate:\n",
    "- **Disruption Risks**: Logistics issues, transportation problems, warehouse challenges\n",
    "- **Supplier Issues**: Supplier bankruptcies, quality problems, capacity constraints\n",
    "- **Geopolitical Risks**: Trade tensions, sanctions, regulatory changes\n",
    "- **Economic Factors**: Inflation, currency fluctuations, commodity price changes\n",
    "\n",
    "### 6. PREDICTIVE INTELLIGENCE\n",
    "Based on the information, predict:\n",
    "- **Short-term Implications** (1-3 months): Immediate business impacts\n",
    "- **Medium-term Trends** (3-12 months): Evolving market dynamics\n",
    "- **Long-term Forecasts** (1-3 years): Strategic industry shifts\n",
    "- **Recommendation Priority**: High/Medium/Low urgency for stakeholder action\n",
    "\n",
    "### 7. BUSINESS IMPACT SCORING\n",
    "Rate the criticality on a scale of 1-10:\n",
    "- **Competitive Impact**: How significantly this affects competitive positioning\n",
    "- **Supply Chain Impact**: Risk level for supply chain operations\n",
    "- **Market Opportunity**: Potential for new business opportunities\n",
    "- **Strategic Importance**: Overall relevance to business strategy\n",
    "\n",
    "\n",
    "## FORMAT\n",
    "\n",
    "### Here's how I'd like you to return my results:\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "Welcome to our weekly newsletter, where we provide you with a brief overview summary of significant events. \n",
    "\n",
    "### Article Summaries:\n",
    "\n",
    "\n",
    "{article_summaries}\n",
    "\n",
    "Stay tuned for our next issue as we delve deeper into the evolving challenges and opportunities shaping the retail landscape.\n",
    "\n",
    "Warm regards,\n",
    "Your retail.ai team\n",
    "\n",
    "\n",
    "\n",
    "##There are several aspects we look for in a good summary:\n",
    " **Consistency**: A good summary should be consistent with the article. Please do not add any new information to the summary, even if you know more about the article's content. For example, if the article does mention the first name of 'President Biden', please do not use the full name 'Joe Biden' in your summary.\n",
    " **Relevance**: A good summary should only include important information from the article. In general, you should include the key points you highlighted in your summary, but feel free to omit details you think are unnecessary to convey the main idea of the article.\n",
    " **Conciseness**: A good summary should be short and to the point. But it is okay if the summary is shorter when the article is short.\n",
    " **Coherence**: A good summary is more than just a list of sentences. It should be a coherent paragraph that makes sense.\n",
    "\n",
    "\n",
    "## TONE\n",
    "Maintain a professional, analytical, and strategic tone throughout your analysis. Be:\n",
    "- **Objective**: Base conclusions on factual evidence from the text\n",
    "- **Actionable**: Provide insights that can drive business decisions\n",
    "- **Comprehensive**: Cover all relevant aspects without being verbose\n",
    "- **Strategic**: Focus on business implications and competitive advantages\n",
    "- **Precise**: Use specific retail/FMCG terminology and metrics\n",
    "\n",
    "\n",
    "## CONSTRAINTS\n",
    "- Only analyze information explicitly mentioned in the provided article\n",
    "- If information is not available for a section, use \"null\" or empty arrays\n",
    "- Maintain consistency in entity naming (e.g., \"Walmart\" not \"Wal-Mart\")\n",
    "- Prioritize actionable insights over general observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Input Articles:\n",
    "```{articles_text}``` \n",
    "\"\"\"\n",
    "###################\n",
    "\n",
    "# Measuring latency\n",
    "start_time = time.time()\n",
    "\n",
    "################\n",
    "# Générer la réponse\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1-mini-2025-04-14\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt_template}],\n",
    "    #prompt=prompt_template,\n",
    "    max_tokens=8096,\n",
    "    top_p=0.5,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "#######################\n",
    "# Calculate latency and total processing time\n",
    "latency = time.time() - start_time\n",
    "#print(chat_completion.choices[0].message.content)\n",
    "#output_text = chat_completion.choices[0].message.content\n",
    "output_text = response.choices[0].message.content\n",
    "\n",
    "# Number of tokens generated in the response\n",
    "output_tokens = len(output_text.split())\n",
    "\n",
    "# Calculate tokens for input and output\n",
    "input_tokens = tokenizer(prompt_template)['input_ids']\n",
    "output_tokens = tokenizer(output_text)['input_ids']\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_input_tokens = len(input_tokens)\n",
    "num_output_tokens = len(output_tokens)\n",
    "\n",
    "# Calculate word length for input and output\n",
    "input_word_count = len(prompt_template.split())\n",
    "output_word_count = len(output_text.split())\n",
    "\n",
    "\n",
    "# Price calculation\n",
    "\n",
    "# Price in USD per million input and output  tokens\n",
    "price_per_million_input_tokens = 0.40  \n",
    "price_per_million_output_tokens = 1.60  \n",
    "\n",
    "\n",
    "# Calculation of entry and exit costs\n",
    "input_cost = (num_input_tokens / 1_000_000) * price_per_million_input_tokens\n",
    "output_cost = (num_output_tokens / 1_000_000) * price_per_million_output_tokens\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "\n",
    "# Define the folder path and create folders if necessary\n",
    "output_dir = os.path.join(\"Output\", \"Ergebnis_test\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the file name with the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"{current_time}_zs_gpt41_mini.txt\"\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Save the result in the text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(output_text)\n",
    "    \n",
    "\n",
    "#print(f\"Output saved to {file_path}\")\n",
    "print(f\"Output saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Function to save the analysis in an Excel file\n",
    "\n",
    "def save_analysis_to_excel(model_name, context_window, output_speed, latency, price, output_price, input_price, time_period, num_input_tokens, num_output_tokens, input_word_count, output_word_count, base_dir=\"LLM_Analysis\"):\n",
    "    data = {\n",
    "        \"Context Window\": [context_window],\n",
    "        \"Output Speed (tokens/s)\": [output_speed],\n",
    "        \"Latency (seconds)\": [latency],\n",
    "        \"Total Cost ($)\": [price],\n",
    "        \"Output Price ($ per M tokens)\": [output_price],\n",
    "        \"Input Price ($ per M tokens)\": [input_price],\n",
    "        \"Time Period (seconds)\": [time_period],\n",
    "        \"Input Tokens\": [num_input_tokens],\n",
    "        \"Output Tokens\": [num_output_tokens],\n",
    "        \"Input Word Count\": [input_word_count],\n",
    "        \"Output Word Count\": [output_word_count]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "   # Create the path to the Excel file\n",
    "    output_dir = os.path.join(base_dir, model_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = f\"{model_name}_analysis_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    try:\n",
    "        df.to_excel(file_path, sheet_name=model_name[:31], index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "           \n",
    "        \n",
    "            \n",
    "\n",
    "# An example of use\n",
    "context_window = 1047576  # Example of the size of the context window\n",
    "output_speed = num_output_tokens / latency  # Tokens per second\n",
    "time_period = latency  # Total time in seconds\n",
    "\n",
    "model_name = f\"zs_gpt41_mini{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "save_analysis_to_excel(\n",
    "    model_name=model_name,\n",
    "    context_window=context_window,\n",
    "    output_speed=output_speed,\n",
    "    latency=latency,\n",
    "    price=total_cost,\n",
    "    output_price=price_per_million_output_tokens,\n",
    "    input_price=price_per_million_input_tokens,\n",
    "    time_period=time_period,\n",
    "    num_input_tokens=num_input_tokens,\n",
    "    num_output_tokens=num_output_tokens,\n",
    "    input_word_count=input_word_count,\n",
    "    output_word_count=output_word_count\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df03e957",
   "metadata": {},
   "source": [
    "## meta-llama/llama-4-maverick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d53daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_with_metadata(df, num_articles=5, offset=0):\n",
    "    articles = df.iloc[offset:offset + num_articles][['url',\n",
    " 'title',\n",
    " 'publish_date',\n",
    " 'authors',\n",
    " 'scraped_at',\n",
    " 'keywords',\n",
    " 'text',\n",
    " 'text_length']].to_dict(orient='records')\n",
    "    return articles\n",
    "\n",
    "# Example to obtain the first 10 articles\n",
    "articles = get_articles_with_metadata(df, num_articles=5, offset=0)\n",
    "\n",
    "# Using metadata to build article text\n",
    "articles_text = \"\\n\\n\".join(\n",
    "    [f\"### Article {i+1}:\\nTitle: {article['title']}\\nURL: {article['url']}\\nPublish_date: {article['publish_date']}\\n\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Dynamically generate the ‘Article Summaries’ section with numbers\n",
    "article_summaries = \"\\n\\n\".join(\n",
    "    [f\"\"\"### {i+1}. \n",
    "     \n",
    "     **Title**: {article['title']}\n",
    "   **Link to source**: {article['url']} \n",
    "   **Publish_date**: {article['publish_date']}\n",
    "   **Summary**: Summarize the key points of the article in 2-3 sentences.\n",
    "   **Category**: Categorize the article into one or more retail-focused categories:[\"product_innovation\", \"pricing_strategy\", \"market_expansion\", \"retail_tech\", \"sustainability\", \"supply_chain_risk\", \"competitor_moves\", \"consumer_trends\", \"others\"].\n",
    "   **Entity Extraction**: Extract and identify brands/compagnies,  products, regions/markets, Financial Data, dates\n",
    "   **sentiments**:Analyze the overall sentiment: Positive, Negative, or Neutral, Market Sentiment, Competitive Sentiment, Supply Chain Sentiment. Provide a brief justification.\n",
    "   **trend detection **:Identify whether the article includes any of the following:Product launches or innovations, Pricing or promotion strategies, Store openings, expansions, or mergers, Retail technology integrations, Sustainability initiatives, Supply chain risks: delays, shortages, geopolitical events\n",
    "   **predictive intelligence **:Predict whether the article suggests:Short-term Implications, Medium-term Trends, Long-term Forecasts, Recommendation Priority, An emerging trend, A supply chain risk, A market shift, Or none\n",
    "   **Business Impact Scoring**: Rate the potential impact of the article on the business on a scale of 1 to 10, where 1 is low impact and 10 is high impact. Provide a brief justification.\n",
    "   \n",
    "   \n",
    "   \"\"\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert retail intelligence analyst working for a major retail/FMCG company that serves 15,000 suppliers and 200 retailers. Your role is to analyze news articles and extract actionable business intelligence that will help stakeholders make informed decisions about competitive positioning, supply chain management, and market opportunities.\n",
    "\n",
    "## ROLE\n",
    "Act as a Senior Retail Intelligence Analyst with expertise in:\n",
    "- Competitive intelligence and market analysis\n",
    "- Supply chain risk assessment\n",
    "- Predictive trend analysis\n",
    "- FMCG/Retail industry dynamics\n",
    "- Business impact evaluation\n",
    "\n",
    "## ACTION\n",
    "Analyze the provided news article and perform the following comprehensive analysis:\n",
    "\n",
    "### 1. INTELLIGENT CLASSIFICATION\n",
    "Categorize the article into ONE primary category:\n",
    "- **product_innovation**: New products, launches, and innovations\n",
    "- **pricing_strategy**: Pricing strategies, promotions, and competitive pricing\n",
    "- **market_expansion**: Store openings, market expansion, and geographic growth\n",
    "- **supply_chain**: Supply chain disruptions, logistics, and sourcing issues\n",
    "- **technology_retail**: Retail technology, digital transformation, and e-commerce\n",
    "- **sustainability**: Sustainability initiatives and ESG practices\n",
    "- **consumer_behavior**: Consumer trends, preferences, and behavioral shifts\n",
    "- **regulatory_compliance**: Regulatory changes, compliance, and policy impacts\n",
    "- **competitive_intelligence**: Competitor moves, market share, and strategic partnerships\n",
    "- **risk_management**: Geopolitical risks, economic factors, and crisis management\n",
    "\n",
    "### 2. ENTITY EXTRACTION\n",
    "Extract and identify:\n",
    "- **Brands/Companies**: All mentioned retail brands, suppliers, competitors\n",
    "- **Products**: Specific products, categories, or product lines\n",
    "- **Regions/Markets**: Geographic locations, countries, cities, markets\n",
    "- **Key Figures**: Important people, executives, industry leaders\n",
    "- **Financial Data**: Revenue, sales figures, market share, investments\n",
    "- **Dates**: Important dates, timelines, deadlines\n",
    "\n",
    "### 3. RETAIL-SPECIFIC SENTIMENT ANALYSIS\n",
    "Assess sentiment across three dimensions:\n",
    "- **Market Sentiment**: Positive/Negative/Neutral impact on market conditions\n",
    "- **Competitive Sentiment**: Advantage/Disadvantage/Neutral for competitive positioning\n",
    "- **Supply Chain Sentiment**: Stable/Risk/Opportunity for supply chain operations\n",
    "\n",
    "### 4. TREND DETECTION & INNOVATION ANALYSIS\n",
    "Identify:\n",
    "- **Emerging Trends**: New consumer behaviors, market shifts, technological adoption\n",
    "- **Innovation Signals**: R&D developments, patent filings, breakthrough technologies\n",
    "- **Product Launches**: New product introductions, line extensions, category expansions\n",
    "- **Market Disruptions**: Business model changes, new market entrants, industry shifts\n",
    "\n",
    "### 5. SUPPLY CHAIN RISK ASSESSMENT\n",
    "Evaluate:\n",
    "- **Disruption Risks**: Logistics issues, transportation problems, warehouse challenges\n",
    "- **Supplier Issues**: Supplier bankruptcies, quality problems, capacity constraints\n",
    "- **Geopolitical Risks**: Trade tensions, sanctions, regulatory changes\n",
    "- **Economic Factors**: Inflation, currency fluctuations, commodity price changes\n",
    "\n",
    "### 6. PREDICTIVE INTELLIGENCE\n",
    "Based on the information, predict:\n",
    "- **Short-term Implications** (1-3 months): Immediate business impacts\n",
    "- **Medium-term Trends** (3-12 months): Evolving market dynamics\n",
    "- **Long-term Forecasts** (1-3 years): Strategic industry shifts\n",
    "- **Recommendation Priority**: High/Medium/Low urgency for stakeholder action\n",
    "\n",
    "### 7. BUSINESS IMPACT SCORING\n",
    "Rate the criticality on a scale of 1-10:\n",
    "- **Competitive Impact**: How significantly this affects competitive positioning\n",
    "- **Supply Chain Impact**: Risk level for supply chain operations\n",
    "- **Market Opportunity**: Potential for new business opportunities\n",
    "- **Strategic Importance**: Overall relevance to business strategy\n",
    "\n",
    "\n",
    "## FORMAT\n",
    "\n",
    "### Here's how I'd like you to return my results:\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "Welcome to our weekly newsletter, where we provide you with a brief overview summary of significant events. \n",
    "\n",
    "### Article Summaries:\n",
    "\n",
    "\n",
    "{article_summaries}\n",
    "\n",
    "Stay tuned for our next issue as we delve deeper into the evolving challenges and opportunities shaping the retail landscape.\n",
    "\n",
    "Warm regards,\n",
    "Your retail.ai team\n",
    "\n",
    "\n",
    "\n",
    "##There are several aspects we look for in a good summary:\n",
    " **Consistency**: A good summary should be consistent with the article. Please do not add any new information to the summary, even if you know more about the article's content. For example, if the article does mention the first name of 'President Biden', please do not use the full name 'Joe Biden' in your summary.\n",
    " **Relevance**: A good summary should only include important information from the article. In general, you should include the key points you highlighted in your summary, but feel free to omit details you think are unnecessary to convey the main idea of the article.\n",
    " **Conciseness**: A good summary should be short and to the point. But it is okay if the summary is shorter when the article is short.\n",
    " **Coherence**: A good summary is more than just a list of sentences. It should be a coherent paragraph that makes sense.\n",
    "\n",
    "\n",
    "## TONE\n",
    "Maintain a professional, analytical, and strategic tone throughout your analysis. Be:\n",
    "- **Objective**: Base conclusions on factual evidence from the text\n",
    "- **Actionable**: Provide insights that can drive business decisions\n",
    "- **Comprehensive**: Cover all relevant aspects without being verbose\n",
    "- **Strategic**: Focus on business implications and competitive advantages\n",
    "- **Precise**: Use specific retail/FMCG terminology and metrics\n",
    "\n",
    "\n",
    "## CONSTRAINTS\n",
    "- Only analyze information explicitly mentioned in the provided article\n",
    "- If information is not available for a section, use \"null\" or empty arrays\n",
    "- Maintain consistency in entity naming (e.g., \"Walmart\" not \"Wal-Mart\")\n",
    "- Prioritize actionable insights over general observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Input Articles:\n",
    "```{articles_text}``` \n",
    "\"\"\"\n",
    "###################\n",
    "\n",
    "# Measuring latency\n",
    "start_time = time.time()\n",
    "\n",
    "################\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"meta-llama/llama-4-maverick-17b-128e-instruct\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt_template}],\n",
    "    #prompt=prompt_template,\n",
    "    max_tokens=8096,\n",
    "    top_p=0.5,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "#######################\n",
    "# Calculate latency and total processing time\n",
    "latency = time.time() - start_time\n",
    "#print(chat_completion.choices[0].message.content)\n",
    "output_text = chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Number of tokens generated in the response\n",
    "output_tokens = len(output_text.split())\n",
    "\n",
    "\n",
    "# Calculate tokens for input and output\n",
    "input_tokens = tokenizer(prompt_template)['input_ids']\n",
    "output_tokens = tokenizer(output_text)['input_ids']\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_input_tokens = len(input_tokens)\n",
    "num_output_tokens = len(output_tokens)\n",
    "\n",
    "# Calculate word length for input and output\n",
    "input_word_count = len(prompt_template.split())\n",
    "output_word_count = len(output_text.split())\n",
    "\n",
    "\n",
    "# Price calculation\n",
    "\n",
    "# Price in USD per million input and output  tokens\n",
    "price_per_million_input_tokens = 0.20  \n",
    "price_per_million_output_tokens = 0.60  \n",
    "\n",
    "\n",
    "# Calculation of entry and exit costs\n",
    "input_cost = (num_input_tokens / 1_000_000) * price_per_million_input_tokens\n",
    "output_cost = (num_output_tokens / 1_000_000) * price_per_million_output_tokens\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "\n",
    "# Define the folder path and create folders if necessary\n",
    "output_dir = os.path.join(\"Output\", \"Ergebnis_test\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the file name with the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"{current_time}_zs_llama4.txt\"\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Save the result in the text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(output_text)\n",
    "    \n",
    "\n",
    "#print(f\"Output saved to {file_path}\")\n",
    "print(f\"Output saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Function to save the analysis in an Excel file\n",
    "\n",
    "def save_analysis_to_excel(model_name, context_window, output_speed, latency, price, output_price, input_price, time_period, num_input_tokens, num_output_tokens, input_word_count, output_word_count, base_dir=\"LLM_Analysis\"):\n",
    "    data = {\n",
    "        \"Context Window\": [context_window],\n",
    "        \"Output Speed (tokens/s)\": [output_speed],\n",
    "        \"Latency (seconds)\": [latency],\n",
    "        \"Total Cost ($)\": [price],\n",
    "        \"Output Price ($ per M tokens)\": [output_price],\n",
    "        \"Input Price ($ per M tokens)\": [input_price],\n",
    "        \"Time Period (seconds)\": [time_period],\n",
    "        \"Input Tokens\": [num_input_tokens],\n",
    "        \"Output Tokens\": [num_output_tokens],\n",
    "        \"Input Word Count\": [input_word_count],\n",
    "        \"Output Word Count\": [output_word_count]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "   # Create the path to the Excel file\n",
    "    output_dir = os.path.join(base_dir, model_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = f\"{model_name}_analysis_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    try:\n",
    "        df.to_excel(file_path, sheet_name=model_name[:31], index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "           \n",
    "        \n",
    "            \n",
    "\n",
    "# An example of use\n",
    "context_window = 131072  #Example of the size of the context window\n",
    "output_speed = num_output_tokens / latency  # Tokens per second\n",
    "time_period = latency  # Total time in seconds\n",
    "\n",
    "model_name = f\"zs_llama4{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "save_analysis_to_excel(\n",
    "    model_name=model_name,\n",
    "    context_window=context_window,\n",
    "    output_speed=output_speed,\n",
    "    latency=latency,\n",
    "    price=total_cost,\n",
    "    output_price=price_per_million_output_tokens,\n",
    "    input_price=price_per_million_input_tokens,\n",
    "    time_period=time_period,\n",
    "    num_input_tokens=num_input_tokens,\n",
    "    num_output_tokens=num_output_tokens,\n",
    "    input_word_count=input_word_count,\n",
    "    output_word_count=output_word_count\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff7e2c",
   "metadata": {},
   "source": [
    "## deepseek-r1-distill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 16:05:48,325 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to Output\\Ergebnis_test\\2025-07-06_16-05-48_zs_deepseek4.txt\n"
     ]
    }
   ],
   "source": [
    "def get_articles_with_metadata(df, num_articles=5, offset=0):\n",
    "    articles = df.iloc[offset:offset + num_articles][['url',\n",
    " 'title',\n",
    " 'publish_date',\n",
    " 'authors',\n",
    " 'scraped_at',\n",
    " 'keywords',\n",
    " 'text',\n",
    " 'text_length']].to_dict(orient='records')\n",
    "    return articles\n",
    "\n",
    "# Example to obtain the first 5 articles\n",
    "articles = get_articles_with_metadata(df, num_articles=5, offset=0)\n",
    "\n",
    "# Using metadata to build article text\n",
    "articles_text = \"\\n\\n\".join(\n",
    "    [f\"### Article {i+1}:\\nTitle: {article['title']}\\nURL: {article['url']}\\nPublish_date: {article['publish_date']}\\n\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Dynamically generate the ‘Article Summaries’ section with numbers\n",
    "article_summaries = \"\\n\\n\".join(\n",
    "    [f\"\"\"### {i+1}. \n",
    "     \n",
    "     **Title**: {article['title']}\n",
    "   **Link to source**: {article['url']} \n",
    "   **Publish_date**: {article['publish_date']}\n",
    "   **Summary**: Summarize the key points of the article in 2-3 sentences.\n",
    "   **Category**: Categorize the article into one or more retail-focused categories:[\"product_innovation\", \"pricing_strategy\", \"market_expansion\", \"retail_tech\", \"sustainability\", \"supply_chain_risk\", \"competitor_moves\", \"consumer_trends\", \"others\"].\n",
    "   **Entity Extraction**: Extract and identify brands/compagnies,  products, regions/markets, Financial Data, dates\n",
    "   **sentiments**:Analyze the overall sentiment: Positive, Negative, or Neutral, Market Sentiment, Competitive Sentiment, Supply Chain Sentiment. Provide a brief justification.\n",
    "   **trend detection **:Identify whether the article includes any of the following:Product launches or innovations, Pricing or promotion strategies, Store openings, expansions, or mergers, Retail technology integrations, Sustainability initiatives, Supply chain risks: delays, shortages, geopolitical events\n",
    "   **predictive intelligence **:Predict whether the article suggests:Short-term Implications, Medium-term Trends, Long-term Forecasts, Recommendation Priority, An emerging trend, A supply chain risk, A market shift, Or none\n",
    "   **Business Impact Scoring**: Rate the potential impact of the article on the business on a scale of 1 to 10, where 1 is low impact and 10 is high impact. Provide a brief justification.\n",
    "   \n",
    "   \n",
    "   \"\"\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert retail intelligence analyst working for a major retail/FMCG company that serves 15,000 suppliers and 200 retailers. Your role is to analyze news articles and extract actionable business intelligence that will help stakeholders make informed decisions about competitive positioning, supply chain management, and market opportunities.\n",
    "\n",
    "## ROLE\n",
    "Act as a Senior Retail Intelligence Analyst with expertise in:\n",
    "- Competitive intelligence and market analysis\n",
    "- Supply chain risk assessment\n",
    "- Predictive trend analysis\n",
    "- FMCG/Retail industry dynamics\n",
    "- Business impact evaluation\n",
    "\n",
    "## ACTION\n",
    "Analyze the provided news article and perform the following comprehensive analysis:\n",
    "\n",
    "### 1. INTELLIGENT CLASSIFICATION\n",
    "Categorize the article into ONE primary category:\n",
    "- **product_innovation**: New products, launches, and innovations\n",
    "- **pricing_strategy**: Pricing strategies, promotions, and competitive pricing\n",
    "- **market_expansion**: Store openings, market expansion, and geographic growth\n",
    "- **supply_chain**: Supply chain disruptions, logistics, and sourcing issues\n",
    "- **technology_retail**: Retail technology, digital transformation, and e-commerce\n",
    "- **sustainability**: Sustainability initiatives and ESG practices\n",
    "- **consumer_behavior**: Consumer trends, preferences, and behavioral shifts\n",
    "- **regulatory_compliance**: Regulatory changes, compliance, and policy impacts\n",
    "- **competitive_intelligence**: Competitor moves, market share, and strategic partnerships\n",
    "- **risk_management**: Geopolitical risks, economic factors, and crisis management\n",
    "\n",
    "### 2. ENTITY EXTRACTION\n",
    "Extract and identify:\n",
    "- **Brands/Companies**: All mentioned retail brands, suppliers, competitors\n",
    "- **Products**: Specific products, categories, or product lines\n",
    "- **Regions/Markets**: Geographic locations, countries, cities, markets\n",
    "- **Key Figures**: Important people, executives, industry leaders\n",
    "- **Financial Data**: Revenue, sales figures, market share, investments\n",
    "- **Dates**: Important dates, timelines, deadlines\n",
    "\n",
    "### 3. RETAIL-SPECIFIC SENTIMENT ANALYSIS\n",
    "Assess sentiment across three dimensions:\n",
    "- **Market Sentiment**: Positive/Negative/Neutral impact on market conditions\n",
    "- **Competitive Sentiment**: Advantage/Disadvantage/Neutral for competitive positioning\n",
    "- **Supply Chain Sentiment**: Stable/Risk/Opportunity for supply chain operations\n",
    "\n",
    "### 4. TREND DETECTION & INNOVATION ANALYSIS\n",
    "Identify:\n",
    "- **Emerging Trends**: New consumer behaviors, market shifts, technological adoption\n",
    "- **Innovation Signals**: R&D developments, patent filings, breakthrough technologies\n",
    "- **Product Launches**: New product introductions, line extensions, category expansions\n",
    "- **Market Disruptions**: Business model changes, new market entrants, industry shifts\n",
    "\n",
    "### 5. SUPPLY CHAIN RISK ASSESSMENT\n",
    "Evaluate:\n",
    "- **Disruption Risks**: Logistics issues, transportation problems, warehouse challenges\n",
    "- **Supplier Issues**: Supplier bankruptcies, quality problems, capacity constraints\n",
    "- **Geopolitical Risks**: Trade tensions, sanctions, regulatory changes\n",
    "- **Economic Factors**: Inflation, currency fluctuations, commodity price changes\n",
    "\n",
    "### 6. PREDICTIVE INTELLIGENCE\n",
    "Based on the information, predict:\n",
    "- **Short-term Implications** (1-3 months): Immediate business impacts\n",
    "- **Medium-term Trends** (3-12 months): Evolving market dynamics\n",
    "- **Long-term Forecasts** (1-3 years): Strategic industry shifts\n",
    "- **Recommendation Priority**: High/Medium/Low urgency for stakeholder action\n",
    "\n",
    "### 7. BUSINESS IMPACT SCORING\n",
    "Rate the criticality on a scale of 1-10:\n",
    "- **Competitive Impact**: How significantly this affects competitive positioning\n",
    "- **Supply Chain Impact**: Risk level for supply chain operations\n",
    "- **Market Opportunity**: Potential for new business opportunities\n",
    "- **Strategic Importance**: Overall relevance to business strategy\n",
    "\n",
    "\n",
    "## FORMAT\n",
    "\n",
    "### Here's how I'd like you to return my results:\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "Welcome to our weekly newsletter, where we provide you with a brief overview summary of significant events. \n",
    "\n",
    "### Article Summaries:\n",
    "\n",
    "\n",
    "{article_summaries}\n",
    "\n",
    "Stay tuned for our next issue as we delve deeper into the evolving challenges and opportunities shaping the retail landscape.\n",
    "\n",
    "Warm regards,\n",
    "Your retail.ai team\n",
    "\n",
    "\n",
    "\n",
    "##There are several aspects we look for in a good summary:\n",
    " **Consistency**: A good summary should be consistent with the article. Please do not add any new information to the summary, even if you know more about the article's content. For example, if the article does mention the first name of 'President Biden', please do not use the full name 'Joe Biden' in your summary.\n",
    " **Relevance**: A good summary should only include important information from the article. In general, you should include the key points you highlighted in your summary, but feel free to omit details you think are unnecessary to convey the main idea of the article.\n",
    " **Conciseness**: A good summary should be short and to the point. But it is okay if the summary is shorter when the article is short.\n",
    " **Coherence**: A good summary is more than just a list of sentences. It should be a coherent paragraph that makes sense.\n",
    "\n",
    "\n",
    "## TONE\n",
    "Maintain a professional, analytical, and strategic tone throughout your analysis. Be:\n",
    "- **Objective**: Base conclusions on factual evidence from the text\n",
    "- **Actionable**: Provide insights that can drive business decisions\n",
    "- **Comprehensive**: Cover all relevant aspects without being verbose\n",
    "- **Strategic**: Focus on business implications and competitive advantages\n",
    "- **Precise**: Use specific retail/FMCG terminology and metrics\n",
    "\n",
    "\n",
    "## CONSTRAINTS\n",
    "- Only analyze information explicitly mentioned in the provided article\n",
    "- If information is not available for a section, use \"null\" or empty arrays\n",
    "- Maintain consistency in entity naming (e.g., \"Walmart\" not \"Wal-Mart\")\n",
    "- Prioritize actionable insights over general observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Input Articles:\n",
    "```{articles_text}``` \n",
    "\"\"\"\n",
    "###################\n",
    "\n",
    "# Measuring latency\n",
    "start_time = time.time()\n",
    "\n",
    "################\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt_template}],\n",
    "    #prompt=prompt_template,\n",
    "    max_tokens=8096,\n",
    "    top_p=0.5,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "#######################\n",
    "# Calculate latency and total processing time\n",
    "latency = time.time() - start_time\n",
    "#print(chat_completion.choices[0].message.content)\n",
    "output_text = chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Number of tokens generated in the response\n",
    "output_tokens = len(output_text.split())\n",
    "\n",
    "\n",
    "# Calculate tokens for input and output\n",
    "input_tokens = tokenizer(prompt_template)['input_ids']\n",
    "output_tokens = tokenizer(output_text)['input_ids']\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_input_tokens = len(input_tokens)\n",
    "num_output_tokens = len(output_tokens)\n",
    "\n",
    "# Calculate word length for input and output\n",
    "input_word_count = len(prompt_template.split())\n",
    "output_word_count = len(output_text.split())\n",
    "\n",
    "\n",
    "# Price calculation\n",
    "\n",
    "# Price in USD per million input and output  tokens\n",
    "price_per_million_input_tokens = 0.75  \n",
    "price_per_million_output_tokens = 0.99  \n",
    "\n",
    "\n",
    "# Calculation of entry and exit costs\n",
    "input_cost = (num_input_tokens / 1_000_000) * price_per_million_input_tokens\n",
    "output_cost = (num_output_tokens / 1_000_000) * price_per_million_output_tokens\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "\n",
    "# Define the folder path and create folders if necessary\n",
    "output_dir = os.path.join(\"Output\", \"Ergebnis_test\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the file name with the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"{current_time}_zs_deepseek4.txt\"\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Save the result in the text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(output_text)\n",
    "    \n",
    "\n",
    "#print(f\"Output saved to {file_path}\")\n",
    "print(f\"Output saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Function to save the analysis in an Excel file\n",
    "\n",
    "def save_analysis_to_excel(model_name, context_window, output_speed, latency, price, output_price, input_price, time_period, num_input_tokens, num_output_tokens, input_word_count, output_word_count, base_dir=\"LLM_Analysis\"):\n",
    "    data = {\n",
    "        \"Context Window\": [context_window],\n",
    "        \"Output Speed (tokens/s)\": [output_speed],\n",
    "        \"Latency (seconds)\": [latency],\n",
    "        \"Total Cost ($)\": [price],\n",
    "        \"Output Price ($ per M tokens)\": [output_price],\n",
    "        \"Input Price ($ per M tokens)\": [input_price],\n",
    "        \"Time Period (seconds)\": [time_period],\n",
    "        \"Input Tokens\": [num_input_tokens],\n",
    "        \"Output Tokens\": [num_output_tokens],\n",
    "        \"Input Word Count\": [input_word_count],\n",
    "        \"Output Word Count\": [output_word_count]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    # Create the path to the Excel file\n",
    "    output_dir = os.path.join(base_dir, model_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = f\"{model_name}_analysis_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    try:\n",
    "        df.to_excel(file_path, sheet_name=model_name[:31], index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "           \n",
    "        \n",
    "            \n",
    "\n",
    "# An example of use\n",
    "context_window = 131072  # Example of the size of the context window\n",
    "output_speed = num_output_tokens / latency  # Tokens per second\n",
    "time_period = latency  # Total time in seconds\n",
    "\n",
    "model_name = f\"zs_deepseek{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "save_analysis_to_excel(\n",
    "    model_name=model_name,\n",
    "    context_window=context_window,\n",
    "    output_speed=output_speed,\n",
    "    latency=latency,\n",
    "    price=total_cost,\n",
    "    output_price=price_per_million_output_tokens,\n",
    "    input_price=price_per_million_input_tokens,\n",
    "    time_period=time_period,\n",
    "    num_input_tokens=num_input_tokens,\n",
    "    num_output_tokens=num_output_tokens,\n",
    "    input_word_count=input_word_count,\n",
    "    output_word_count=output_word_count\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b349e",
   "metadata": {},
   "source": [
    "## qwen/qwen3-32b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8373c502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-06 16:09:53,908 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to Output\\Ergebnis_test\\2025-07-06_16-09-54_zs_qwen.txt\n"
     ]
    }
   ],
   "source": [
    "def get_articles_with_metadata(df, num_articles=5, offset=0):\n",
    "    articles = df.iloc[offset:offset + num_articles][['url',\n",
    " 'title',\n",
    " 'publish_date',\n",
    " 'authors',\n",
    " 'scraped_at',\n",
    " 'keywords',\n",
    " 'text',\n",
    " 'text_length']].to_dict(orient='records')\n",
    "    return articles\n",
    "\n",
    "# Example to obtain the first 5 articles\n",
    "articles = get_articles_with_metadata(df, num_articles=5, offset=0)\n",
    "\n",
    "# Using metadata to build article text\n",
    "articles_text = \"\\n\\n\".join(\n",
    "    [f\"### Article {i+1}:\\nTitle: {article['title']}\\nURL: {article['url']}\\nPublish_date: {article['publish_date']}\\n\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Dynamically generate the ‘Article Summaries’ section with numbers\n",
    "article_summaries = \"\\n\\n\".join(\n",
    "    [f\"\"\"### {i+1}. \n",
    "     \n",
    "     **Title**: {article['title']}\n",
    "   **Link to source**: {article['url']} \n",
    "   **Publish_date**: {article['publish_date']}\n",
    "   **Summary**: Summarize the key points of the article in 2-3 sentences.\n",
    "   **Category**: Categorize the article into one or more retail-focused categories:[\"product_innovation\", \"pricing_strategy\", \"market_expansion\", \"retail_tech\", \"sustainability\", \"supply_chain_risk\", \"competitor_moves\", \"consumer_trends\", \"others\"].\n",
    "   **Entity Extraction**: Extract and identify brands/compagnies,  products, regions/markets, Financial Data, dates\n",
    "   **sentiments**:Analyze the overall sentiment: Positive, Negative, or Neutral, Market Sentiment, Competitive Sentiment, Supply Chain Sentiment. Provide a brief justification.\n",
    "   **trend detection **:Identify whether the article includes any of the following:Product launches or innovations, Pricing or promotion strategies, Store openings, expansions, or mergers, Retail technology integrations, Sustainability initiatives, Supply chain risks: delays, shortages, geopolitical events\n",
    "   **predictive intelligence **:Predict whether the article suggests:Short-term Implications, Medium-term Trends, Long-term Forecasts, Recommendation Priority, An emerging trend, A supply chain risk, A market shift, Or none\n",
    "   **Business Impact Scoring**: Rate the potential impact of the article on the business on a scale of 1 to 10, where 1 is low impact and 10 is high impact. Provide a brief justification.\n",
    "   \n",
    "   \n",
    "   \"\"\"\n",
    "     for i, article in enumerate(articles)]\n",
    ")\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = f\"\"\"\n",
    "You are an expert retail intelligence analyst working for a major retail/FMCG company that serves 15,000 suppliers and 200 retailers. Your role is to analyze news articles and extract actionable business intelligence that will help stakeholders make informed decisions about competitive positioning, supply chain management, and market opportunities.\n",
    "\n",
    "## ROLE\n",
    "Act as a Senior Retail Intelligence Analyst with expertise in:\n",
    "- Competitive intelligence and market analysis\n",
    "- Supply chain risk assessment\n",
    "- Predictive trend analysis\n",
    "- FMCG/Retail industry dynamics\n",
    "- Business impact evaluation\n",
    "\n",
    "## ACTION\n",
    "Analyze the provided news article and perform the following comprehensive analysis:\n",
    "\n",
    "### 1. INTELLIGENT CLASSIFICATION\n",
    "Categorize the article into ONE primary category:\n",
    "- **product_innovation**: New products, launches, and innovations\n",
    "- **pricing_strategy**: Pricing strategies, promotions, and competitive pricing\n",
    "- **market_expansion**: Store openings, market expansion, and geographic growth\n",
    "- **supply_chain**: Supply chain disruptions, logistics, and sourcing issues\n",
    "- **technology_retail**: Retail technology, digital transformation, and e-commerce\n",
    "- **sustainability**: Sustainability initiatives and ESG practices\n",
    "- **consumer_behavior**: Consumer trends, preferences, and behavioral shifts\n",
    "- **regulatory_compliance**: Regulatory changes, compliance, and policy impacts\n",
    "- **competitive_intelligence**: Competitor moves, market share, and strategic partnerships\n",
    "- **risk_management**: Geopolitical risks, economic factors, and crisis management\n",
    "\n",
    "### 2. ENTITY EXTRACTION\n",
    "Extract and identify:\n",
    "- **Brands/Companies**: All mentioned retail brands, suppliers, competitors\n",
    "- **Products**: Specific products, categories, or product lines\n",
    "- **Regions/Markets**: Geographic locations, countries, cities, markets\n",
    "- **Key Figures**: Important people, executives, industry leaders\n",
    "- **Financial Data**: Revenue, sales figures, market share, investments\n",
    "- **Dates**: Important dates, timelines, deadlines\n",
    "\n",
    "### 3. RETAIL-SPECIFIC SENTIMENT ANALYSIS\n",
    "Assess sentiment across three dimensions:\n",
    "- **Market Sentiment**: Positive/Negative/Neutral impact on market conditions\n",
    "- **Competitive Sentiment**: Advantage/Disadvantage/Neutral for competitive positioning\n",
    "- **Supply Chain Sentiment**: Stable/Risk/Opportunity for supply chain operations\n",
    "\n",
    "### 4. TREND DETECTION & INNOVATION ANALYSIS\n",
    "Identify:\n",
    "- **Emerging Trends**: New consumer behaviors, market shifts, technological adoption\n",
    "- **Innovation Signals**: R&D developments, patent filings, breakthrough technologies\n",
    "- **Product Launches**: New product introductions, line extensions, category expansions\n",
    "- **Market Disruptions**: Business model changes, new market entrants, industry shifts\n",
    "\n",
    "### 5. SUPPLY CHAIN RISK ASSESSMENT\n",
    "Evaluate:\n",
    "- **Disruption Risks**: Logistics issues, transportation problems, warehouse challenges\n",
    "- **Supplier Issues**: Supplier bankruptcies, quality problems, capacity constraints\n",
    "- **Geopolitical Risks**: Trade tensions, sanctions, regulatory changes\n",
    "- **Economic Factors**: Inflation, currency fluctuations, commodity price changes\n",
    "\n",
    "### 6. PREDICTIVE INTELLIGENCE\n",
    "Based on the information, predict:\n",
    "- **Short-term Implications** (1-3 months): Immediate business impacts\n",
    "- **Medium-term Trends** (3-12 months): Evolving market dynamics\n",
    "- **Long-term Forecasts** (1-3 years): Strategic industry shifts\n",
    "- **Recommendation Priority**: High/Medium/Low urgency for stakeholder action\n",
    "\n",
    "### 7. BUSINESS IMPACT SCORING\n",
    "Rate the criticality on a scale of 1-10:\n",
    "- **Competitive Impact**: How significantly this affects competitive positioning\n",
    "- **Supply Chain Impact**: Risk level for supply chain operations\n",
    "- **Market Opportunity**: Potential for new business opportunities\n",
    "- **Strategic Importance**: Overall relevance to business strategy\n",
    "\n",
    "\n",
    "## FORMAT\n",
    "\n",
    "### Here's how I'd like you to return my results:\n",
    "\n",
    "Dear Customer,\n",
    "\n",
    "Welcome to our weekly newsletter, where we provide you with a brief overview summary of significant events. \n",
    "\n",
    "### Article Summaries:\n",
    "\n",
    "\n",
    "{article_summaries}\n",
    "\n",
    "Stay tuned for our next issue as we delve deeper into the evolving challenges and opportunities shaping the retail landscape.\n",
    "\n",
    "Warm regards,\n",
    "Your retail.ai team\n",
    "\n",
    "\n",
    "\n",
    "##There are several aspects we look for in a good summary:\n",
    " **Consistency**: A good summary should be consistent with the article. Please do not add any new information to the summary, even if you know more about the article's content. For example, if the article does mention the first name of 'President Biden', please do not use the full name 'Joe Biden' in your summary.\n",
    " **Relevance**: A good summary should only include important information from the article. In general, you should include the key points you highlighted in your summary, but feel free to omit details you think are unnecessary to convey the main idea of the article.\n",
    " **Conciseness**: A good summary should be short and to the point. But it is okay if the summary is shorter when the article is short.\n",
    " **Coherence**: A good summary is more than just a list of sentences. It should be a coherent paragraph that makes sense.\n",
    "\n",
    "\n",
    "## TONE\n",
    "Maintain a professional, analytical, and strategic tone throughout your analysis. Be:\n",
    "- **Objective**: Base conclusions on factual evidence from the text\n",
    "- **Actionable**: Provide insights that can drive business decisions\n",
    "- **Comprehensive**: Cover all relevant aspects without being verbose\n",
    "- **Strategic**: Focus on business implications and competitive advantages\n",
    "- **Precise**: Use specific retail/FMCG terminology and metrics\n",
    "\n",
    "\n",
    "## CONSTRAINTS\n",
    "- Only analyze information explicitly mentioned in the provided article\n",
    "- If information is not available for a section, use \"null\" or empty arrays\n",
    "- Maintain consistency in entity naming (e.g., \"Walmart\" not \"Wal-Mart\")\n",
    "- Prioritize actionable insights over general observations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Input Articles:\n",
    "```{articles_text}``` \n",
    "\"\"\"\n",
    "###################\n",
    "\n",
    "# Measuring latency\n",
    "start_time = time.time()\n",
    "\n",
    "################\n",
    "chat_completion = client.chat.completions.create(\n",
    "    model=\"deepseek-r1-distill-llama-70b\",\n",
    "    messages=[{\"role\":\"user\",\"content\":prompt_template}],\n",
    "    #prompt=prompt_template,\n",
    "    max_tokens=8096,\n",
    "    top_p=0.5,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "#######################\n",
    "# Calculate latency and total processing time\n",
    "latency = time.time() - start_time\n",
    "#print(chat_completion.choices[0].message.content)\n",
    "output_text = chat_completion.choices[0].message.content\n",
    "\n",
    "\n",
    "# Number of tokens generated in the response\n",
    "output_tokens = len(output_text.split())\n",
    "\n",
    "\n",
    "# Calculate tokens for input and output\n",
    "input_tokens = tokenizer(prompt_template)['input_ids']\n",
    "output_tokens = tokenizer(output_text)['input_ids']\n",
    "\n",
    "# Calculate the number of tokens\n",
    "num_input_tokens = len(input_tokens)\n",
    "num_output_tokens = len(output_tokens)\n",
    "\n",
    "# Calculate word length for input and output\n",
    "input_word_count = len(prompt_template.split())\n",
    "output_word_count = len(output_text.split())\n",
    "\n",
    "\n",
    "# Price calculation\n",
    "\n",
    "# Price in USD per million input and output  tokens\n",
    "price_per_million_input_tokens = 0.29  \n",
    "price_per_million_output_tokens = 0.59  \n",
    "\n",
    "\n",
    "# Calculation of entry and exit costs\n",
    "input_cost = (num_input_tokens / 1_000_000) * price_per_million_input_tokens\n",
    "output_cost = (num_output_tokens / 1_000_000) * price_per_million_output_tokens\n",
    "total_cost = input_cost + output_cost\n",
    "\n",
    "\n",
    "# Define the folder path and create folders if necessary\n",
    "output_dir = os.path.join(\"Output\", \"Ergebnis_test\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the file name with the current date and time\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "file_name = f\"{current_time}_zs_qwen.txt\"\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Save the result in the text file\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(output_text)\n",
    "    \n",
    "\n",
    "#print(f\"Output saved to {file_path}\")\n",
    "print(f\"Output saved to {file_path}\")\n",
    "\n",
    "\n",
    "# Function to save the analysis in an Excel file\n",
    "\n",
    "def save_analysis_to_excel(model_name, context_window, output_speed, latency, price, output_price, input_price, time_period, num_input_tokens, num_output_tokens, input_word_count, output_word_count, base_dir=\"LLM_Analysis\"):\n",
    "    data = {\n",
    "        \"Context Window\": [context_window],\n",
    "        \"Output Speed (tokens/s)\": [output_speed],\n",
    "        \"Latency (seconds)\": [latency],\n",
    "        \"Total Cost ($)\": [price],\n",
    "        \"Output Price ($ per M tokens)\": [output_price],\n",
    "        \"Input Price ($ per M tokens)\": [input_price],\n",
    "        \"Time Period (seconds)\": [time_period],\n",
    "        \"Input Tokens\": [num_input_tokens],\n",
    "        \"Output Tokens\": [num_output_tokens],\n",
    "        \"Input Word Count\": [input_word_count],\n",
    "        \"Output Word Count\": [output_word_count]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    # Create the path to the Excel file\n",
    "    output_dir = os.path.join(base_dir, model_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = f\"{model_name}_analysis_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    try:\n",
    "        df.to_excel(file_path, sheet_name=model_name[:31], index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "           \n",
    "        \n",
    "            \n",
    "\n",
    "# An example of use\n",
    "context_window = 131072  # Example of the size of the context window\n",
    "output_speed = num_output_tokens / latency  # Tokens per second\n",
    "time_period = latency  # Total time in seconds\n",
    "\n",
    "model_name = f\"zs_qwen{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "save_analysis_to_excel(\n",
    "    model_name=model_name,\n",
    "    context_window=context_window,\n",
    "    output_speed=output_speed,\n",
    "    latency=latency,\n",
    "    price=total_cost,\n",
    "    output_price=price_per_million_output_tokens,\n",
    "    input_price=price_per_million_input_tokens,\n",
    "    time_period=time_period,\n",
    "    num_input_tokens=num_input_tokens,\n",
    "    num_output_tokens=num_output_tokens,\n",
    "    input_word_count=input_word_count,\n",
    "    output_word_count=output_word_count\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f40829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d2d1e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
